{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "\n",
    "import data.patch2D\n",
    "import data.patch3D\n",
    "\n",
    "import models.layers.morphology\n",
    "import models.custom_backbone\n",
    "import models.custom_architecture\n",
    "import tism\n",
    "\n",
    "import metrics.connected_components\n",
    "import metrics.distance_contour\n",
    "\n",
    "from utils.clean_string import clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ipykernel' in sys.modules:\n",
    "    verbose = 1\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"2\", \"UNET\", \"32\", \"(256, 256, 1)\", \"16\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"2\", \"UNET_MM_ALPHA\", \"32\", \"(256, 256, 1)\", \"16\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"2\", \"UNET_MM_GAMMA\", \"32\", \"(256, 256, 1)\", \"16\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"2\", \"UNET_MM_BETA\", \"32\", \"(256, 256, 1)\", \"16\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"REU_IGBMC_9C\", \"3\", \"UNET4\", \"32\", \"(16, 128, 128, 1)\", \"8\", \"MSE_DT\", \"LW4_40_9\"]\n",
    "    # params = [\"REU_IGBMC_9C\", \"2\", \"UNET\", \"32\", \"(128, 128, 1)\", \"64\", \"MSE_DT\", \"LW4_40_9\"]\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"2\", \"UNETMULT\", \"32\", \"(256, 256, 1)\", \"16\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"MODEL_ARCHITECTURE\", \"3\", \"UNETMULT\", \"32\", \"(32,256,256,1)\", \"1\", \"MSE_DT_20_20\", \"MitoEM\"]\n",
    "else:\n",
    "    verbose = 0\n",
    "    params = sys.argv[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp params\n",
    "# params = [\"str_expname\", \"int_model_dim\", \"str_modelname\", \"str_model_param\", \"tuple_input_size\", \"int_batch_size\", \"str_lossname\", \"str_dataset\"]\n",
    "N_PARAMS = 8\n",
    "if not (len(params) == N_PARAMS):\n",
    "    print(\"error: number of param\", flush=True)\n",
    "    exit(1)\n",
    "\n",
    "EXP_NAME = str(params[0])\n",
    "MODEL_DIM = int(params[1])\n",
    "MODEL_NAME = str(params[2])\n",
    "MODEL_PARAM = int(params[3])\n",
    "PATCH_SIZE = tuple(map(int, params[4].replace(\"(\",\"\").replace(\")\",\"\").split(','))) \n",
    "BATCH_SIZE = int(params[5])\n",
    "LOSS = str(params[6])\n",
    "DATASET = str(params[7])\n",
    "\n",
    "\n",
    "if MODEL_DIM not in (2, 3):\n",
    "    print(\"error: model dimension must be equal to 2 or 3\", flush=True)\n",
    "    exit(1)\n",
    "\n",
    "# Fixed params\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "OUTPUT_ACT = 'sigmoid'\n",
    "BINARY_THRESHOLD = 0.5\n",
    "if \"DT\" in LOSS:\n",
    "    OUTPUT_ACT = 'tanh'\n",
    "    BINARY_THRESHOLD = 0\n",
    "\n",
    "EPOCHS = 750\n",
    "TRAIN_PER_EPOCHS = 300\n",
    "VALID_PER_EPOCHS = 100\n",
    "EARLY_PATIENCE = 20\n",
    "REDUCE_PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp related computing\n",
    "if MODEL_DIM == 3:\n",
    "    input_shape=(None, None, None, 1)\n",
    "else:\n",
    "    input_shape=(None, None, 1)\n",
    "\n",
    "be = tism.backbone.VGG(initial_block_depth=MODEL_PARAM, initial_block_length=2, batch_normalization=True)\n",
    "bd = tism.backbone.VGG(initial_block_depth=MODEL_PARAM, initial_block_length=2, batch_normalization=True)\n",
    "\n",
    "if   MODEL_NAME == \"UNET\":\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNET4\":\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=4, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNET_RES\":\n",
    "    be = tism.backbone.ResBlock(backbone=be)\n",
    "    bd = tism.backbone.ResBlock(backbone=bd)\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"LINKNET\":\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.LinkNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"LINKNET_RES\":\n",
    "    be = tism.backbone.ResBlock(backbone=be)\n",
    "    bd = tism.backbone.ResBlock(backbone=bd)\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.LinkNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNET_MM_ALPHA\":\n",
    "    be = models.custom_backbone.MM_Alpha(backbone=be)\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNET_MM_BETA\":\n",
    "    be = models.custom_backbone.MM_Beta(backbone=be)\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNET_MM_GAMMA\":\n",
    "    be = models.custom_backbone.MM_Gamma(backbone=be)\n",
    "    MODEL = tism.model.get(architecture=tism.architecture.UNet(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "elif MODEL_NAME == \"UNETMULT\":\n",
    "    MODEL = tism.model.get(architecture=models.custom_architecture.UNetMultiply(input_shape=input_shape, depth=5, output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT, op_dim=MODEL_DIM, dropout=0.50),\n",
    "                          backbone_encoder=be,\n",
    "                          backbone_decoder=bd)\n",
    "else:\n",
    "    print(\"error: model does not exist\", flush=True)\n",
    "    exit(1)\n",
    "\n",
    "print(EXP_NAME, str(MODEL_DIM) + \"D\", MODEL_NAME, MODEL_PARAM, PATCH_SIZE, BATCH_SIZE, LOSS, DATASET, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"I3\":\n",
    "    import data.datasets.I3 as D\n",
    "elif DATASET == \"LW4\":\n",
    "    import data.datasets.LW4 as D\n",
    "elif DATASET == \"LW4_40_9\":\n",
    "    import data.datasets.LW4_40_9 as D\n",
    "elif DATASET == \"MitoEM\":\n",
    "    if LOSS == \"MSE_DT\":\n",
    "        import data.datasets.MitoEM_dt as D\n",
    "        D.load_dt(20, 20)\n",
    "    else:\n",
    "        import data.datasets.MitoEM as D\n",
    "elif DATASET == \"MitoEM-H\":\n",
    "    if \"DT\" in LOSS:\n",
    "        import data.datasets.MitoEMH_dt as D\n",
    "        s = LOSS.split(\"_\")\n",
    "        if len(s) == 4:\n",
    "            D.load_dt(int(s[2]), int(s[3]))\n",
    "        else:\n",
    "            D.load_dt(20, 20)\n",
    "    else:\n",
    "        import data.datasets.MitoEMH as D\n",
    "elif DATASET == \"MitoEM-R\":\n",
    "    if \"DT\" in LOSS:\n",
    "        import data.datasets.MitoEMR_dt as D\n",
    "        D.load_dt(20, 20)\n",
    "    else:\n",
    "        import data.datasets.MitoEMR as D\n",
    "else:\n",
    "    print(\"error: dataset does not exist\", flush=True)\n",
    "    exit(1)\n",
    "\n",
    "if(os.uname()[1] == 'lythandas'):\n",
    "    OUTPUT_FOLDER = \"/home/cyril/Development/NeNISt/\" + EXP_NAME\n",
    "else:\n",
    "    OUTPUT_FOLDER = \"/b/home/miv/cmeyer/NeNISt/\" + EXP_NAME\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.today().strftime(\"%j%H%M%S%f\")[:-2]\n",
    "\n",
    "CURRENT_EXP_NAME = clean_string(str(MODEL_DIM) + \"D\" + \"_\" + str(MODEL_NAME) + \"_\" + str(MODEL_PARAM) + \"_\" + str(PATCH_SIZE) + \"_\" + str(BATCH_SIZE) + \"_\" + LOSS + \"_\" + DATASET + \"_\" + dt).replace(\",\",\"x\")\n",
    "print(CURRENT_EXP_NAME, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"LW4_40_9\":\n",
    "    train_image = D.train_image_normalized_f16\n",
    "    #train_labels_dt = D.train_labels_dt\n",
    "    #train_labels_indexes = [D.train_label_1_indexes, D.train_label_2_indexes, D.train_label_3_indexes, D.train_label_4_indexes, D.train_label_5_indexes, D.train_label_6_indexes, D.train_label_7_indexes, D.train_label_8_indexes, D.train_label_9_indexes]\n",
    "    valid_image = D.test_image_normalized_f16\n",
    "    #test_labels_dt = D.test_labels_dt\n",
    "    #test_labels_indexes = [D.test_label_1_indexes, D.test_label_2_indexes, D.test_label_3_indexes, D.test_label_4_indexes, D.test_label_5_indexes, D.test_label_6_indexes, D.test_label_7_indexes, D.test_label_8_indexes, D.test_label_9_indexes]\n",
    "    train_label = D.train_labels_dt\n",
    "    valid_label = D.test_labels_dt\n",
    "\n",
    "elif DATASET in [\"MitoEM\", \"MitoEM-R\", \"MitoEM-H\"]:\n",
    "    train_image = D.train_image_normalized_f16\n",
    "    train_label = D.train_label\n",
    "    valid_image = D.valid_image_normalized_f16\n",
    "    valid_label = D.valid_label\n",
    "\n",
    "    # case when X and Z axis are echanged\n",
    "    '''\n",
    "    if not PATCH_SIZE[-1] == 1:\n",
    "        train_image = np.moveaxis(train_image, 0, 2)\n",
    "        train_label = np.moveaxis(train_label, 0, 2)\n",
    "        valid_image = np.moveaxis(valid_image, 0, 2)\n",
    "        valid_label = np.moveaxis(valid_label, 0, 2)\n",
    "    '''\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "'''\n",
    "# compute class weights on train dataset\n",
    "train_labels_one_hot = train_labels_dt > 0\n",
    "\n",
    "# class weight ([0, 1])\n",
    "class_weights = np.zeros(train_labels_one_hot.shape[-1])\n",
    "for c in range(len(class_weights)):\n",
    "    class_weights[c] = 1 - train_labels_one_hot[:,:,:,c].sum() / (train_labels_one_hot.shape[0] * train_labels_one_hot.shape[1] * train_labels_one_hot.shape[2])\n",
    "print(np.round(class_weights, 2))\n",
    "\n",
    "# class weight : wj=n_samples / (n_classes * n_samplesj)\n",
    "class_weights = np.zeros(train_labels_one_hot.shape[-1])\n",
    "for c in range(len(class_weights)):\n",
    "    class_weights[c] = (train_labels_one_hot.shape[0] * train_labels_one_hot.shape[1] * train_labels_one_hot.shape[2]) / (train_labels_one_hot.shape[3] * train_labels_one_hot[:,:,:,c].sum())\n",
    "print(np.round(class_weights, 2))\n",
    "'''\n",
    "# only run once, results :\n",
    "# BCE WEIGHTS: [0.52861168 9.23769132]\n",
    "'''\n",
    "if LOSS == \"BCE\":\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "                                            classes=[0,1],\n",
    "                                            y=train_label[0:10].flatten())\n",
    "    print(\"BCE WEIGHTS:\", weights)\n",
    "'''\n",
    "'''\n",
    "weights = None\n",
    "if LOSS == \"BCE\":\n",
    "    weights = np.array([0.5, 10.0])\n",
    "'''\n",
    "weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "'''\n",
    "if \"3D\" in MODELNAME and len(PATCH_SIZE) == 4:\n",
    "    train = data.patch3D.gen_patches_batch_augmented_3d_label_indexes_one_hot(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], train_image, train_labels_dt, train_labels_indexes, batch_size=BATCH_SIZE)\n",
    "    test = data.patch3D.gen_patches_batch_augmented_3d_label_indexes_one_hot(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], test_image, test_labels_dt, test_labels_indexes, batch_size=BATCH_SIZE)\n",
    "elif \"2D\" in MODELNAME and len(PATCH_SIZE) == 3:\n",
    "    if PATCH_SIZE[0] == PATCH_SIZE[1]:\n",
    "        train = data.patch2D.gen_patches_batch_augmented_label_indexes_one_hot(PATCH_SIZE[0], train_image, train_labels_dt, train_labels_indexes, batch_size=BATCH_SIZE)\n",
    "        test = data.patch2D.gen_patches_batch_augmented_label_indexes_one_hot(PATCH_SIZE[0], test_image, test_labels_dt, test_labels_indexes, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        print(\"error: non square 2D patch size, check data.patch2D\")\n",
    "        exit(1)\n",
    "else:\n",
    "    print(\"error: patch size the model are not compatible\")\n",
    "    exit(1)\n",
    "'''\n",
    "\n",
    "if OUTPUT_CLASSES > 2:\n",
    "    if len(PATCH_SIZE) == 3:\n",
    "        train = data.patch2D.gen_patches_batch_augmented_2d(PATCH_SIZE[0], PATCH_SIZE[1], OUTPUT_CLASSES, train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch2D.gen_patches_batch_augmented_2d(PATCH_SIZE[0], PATCH_SIZE[1], OUTPUT_CLASSES, valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    elif len(PATCH_SIZE) == 4:\n",
    "        train = data.patch3D.gen_patches_batch_augmented_3d(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], OUTPUT_CLASSES, train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch3D.gen_patches_batch_augmented_3d(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], OUTPUT_CLASSES, valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    else:\n",
    "        print(\"error: patch size\", flush=True)\n",
    "        exit(1)\n",
    "else:\n",
    "    if len(PATCH_SIZE) == 3:\n",
    "        train = data.patch2D.gen_patches_batch_augmented_2d_bin(PATCH_SIZE[0], PATCH_SIZE[1], train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch2D.gen_patches_batch_augmented_2d_bin(PATCH_SIZE[0], PATCH_SIZE[1], valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    elif len(PATCH_SIZE) == 4:\n",
    "        train = data.patch3D.gen_patches_batch_augmented_3d_bin(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch3D.gen_patches_batch_augmented_3d_bin(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    else:\n",
    "        print(\"error: patch size\", flush=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "from tensorflow.keras import backend as K\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    # https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    # https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "if \"MSE\" in LOSS:\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "elif \"MAE_DT\" in LOSS:\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "elif LOSS == \"DICE\":\n",
    "    loss = dice_coef_loss\n",
    "elif LOSS == \"JACCARD\":\n",
    "    loss = jaccard_distance_loss\n",
    "elif LOSS == \"SM_DICE\":\n",
    "    loss = sm.losses.dice_loss\n",
    "elif LOSS == \"SM_JACCARD\":\n",
    "    loss = sm.losses.jaccard_loss\n",
    "elif LOSS == \"SM_BIN_FOCAL\":\n",
    "    loss = sm.losses.binary_focal_loss\n",
    "elif LOSS == \"BCE\":\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "else:\n",
    "    print(\"error: loss name\", flush=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "model = MODEL\n",
    "model.compile(optimizer=optimizer, loss=loss) # metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.BinaryCrossentropy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlystopping = tf.keras.callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience=EARLY_PATIENCE, restore_best_weights = True)\n",
    "reducelrplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=REDUCE_PATIENCE)\n",
    "checkpoint_path = OUTPUT_FOLDER + \"/\" + CURRENT_EXP_NAME + \".h5\"\n",
    "savebestmodel = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# fast test\n",
    "if 'ipykernel' in sys.modules:\n",
    "    EPOCHS = 10\n",
    "    TRAIN_PER_EPOCHS = 64\n",
    "    VALID_PER_EPOCHS = 6\n",
    "\n",
    "fit_history = model.fit(train, steps_per_epoch=TRAIN_PER_EPOCHS, epochs=EPOCHS,\n",
    "                        validation_data=valid, validation_steps=VALID_PER_EPOCHS,\n",
    "                        verbose=verbose, callbacks=[reducelrplateau, savebestmodel])\n",
    "\n",
    "t1 = time.time()\n",
    "train_time = int(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not savebestmodel\n",
    "# checkpoint_path = OUTPUT_FOLDER + \"/\" + CURRENT_EXP_NAME + \".h5\"\n",
    "# model.save_weights(checkpoint_path)\n",
    "\n",
    "# model.save_weights(OUTPUT_FOLDER + \"/\" + CURRENT_EXP_NAME + \".h5\")\n",
    "# model.evaluate(valid, steps=TRAIN_PER_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(checkpoint_path)\n",
    "# model.save(OUTPUT_FOLDER + \"/MODEL_\" + CURRENT_EXP_NAME + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = len(fit_history.history['loss'])\n",
    "f_history_name = OUTPUT_FOLDER + \"/\" + clean_string(CURRENT_EXP_NAME + \"_history_\" + str(n_epochs)) + \".txt\"\n",
    "f = open(f_history_name, \"w\")\n",
    "f.write(str(fit_history.history))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_results_name = OUTPUT_FOLDER + \"/\" + clean_string(CURRENT_EXP_NAME) + \".csv\"\n",
    "f_results = open(f_results_name, \"w\")\n",
    "if len(PATCH_SIZE) == 3 and PATCH_SIZE[-1] == 1:\n",
    "    f_results.write(\"modelname,train_time,ACC_2048,IOU_2048,F1_2048,MEAN_DIST_GD_2048,MEAN_DIST_PRED_2048,CC_TP_2048,CC_FN_2048,CC_UD_2048,ACC_384,IOU_384,F1_384\\n\")\n",
    "elif len(PATCH_SIZE) == 3:\n",
    "    f_results.write(\"modelname,train_time,ACC_384,IOU_384,F1_384,MEAN_DIST_GD_384,MEAN_DIST_PRED_384,CC_TP_384,CC_FN_384,CC_UD_384\\n\")\n",
    "elif len(PATCH_SIZE) == 4:\n",
    "    f_results.write(\"modelname,train_time,ACC_384,IOU_384,F1_384,MEAN_DIST_GD_384,MEAN_DIST_PRED_384,CC_TP_384,CC_FN_384,CC_UD_384\\n\")\n",
    "f_results.close()\n",
    "\n",
    "modelname = clean_string(str(MODEL_DIM) + \"D\" + \"_\" + str(MODEL_NAME) + \"_\" + str(MODEL_PARAM) + \"_\" + str(PATCH_SIZE) + \"_\" + str(BATCH_SIZE) + \"_\" + LOSS + \"_\" + DATASET).replace(\",\",\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = valid_image\n",
    "test_label = valid_label\n",
    "if PATCH_SIZE[-1] == 1:\n",
    "    test_section_label = test_label[10:10+50, 1250:1250+1500, 1250:1250+1500]\n",
    "    test_section_pred  = np.zeros(test_section_label.shape, dtype=test_section_label.dtype)\n",
    "else:\n",
    "    test_section_label = test_label[1250:1250+1500, 1250:1250+1500, 10:10+50]\n",
    "    test_section_pred  = np.zeros(test_section_label.shape, dtype=test_section_label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof\n",
    "'''\n",
    "for z in range(50):\n",
    "    p = np.expand_dims(np.expand_dims(test_label[z+10, 1000:1000+2048, 1000:1000+2048], -1), 0)\n",
    "    p = p[0,250:250+1500,250:250+1500,0]\n",
    "    test_section_pred[z] = p\n",
    "print((test_section_pred == test_section_label).all())\n",
    "\n",
    "for z in range(50):\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(np.expand_dims(test_label[z+10, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "            p = p[0, :, :, 0]\n",
    "            \n",
    "            if x_ == 0:\n",
    "                p = p[:, pad:]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:, :-pad]\n",
    "            if y_ == 0:\n",
    "                p = p[pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:-pad, :]\n",
    "\n",
    "            test_section_pred[z, y:y+p.shape[0], x:x+p.shape[1]] = p[:, :]\n",
    "\n",
    "print((test_section_pred == test_section_label).all())\n",
    "\n",
    "\n",
    "for y_ in range(4):\n",
    "    for x_ in range(4):\n",
    "        x = x_*384\n",
    "        y = y_*384\n",
    "        pad = 18\n",
    "        p = np.expand_dims(np.expand_dims(test_label[0:64, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "        \n",
    "        p = p[0,10:10+50, :, :, 0]\n",
    "\n",
    "        if x_ == 0:\n",
    "            p = p[:, :, pad:]\n",
    "        else:\n",
    "            x = x-pad\n",
    "        if x_ == 3:\n",
    "            p = p[:, :, :-pad]\n",
    "        if y_ == 0:\n",
    "            p = p[:, pad:, :]\n",
    "        else:\n",
    "            y = y-pad\n",
    "        if y_ == 3:\n",
    "            p = p[:, :-pad, :]\n",
    "        \n",
    "        test_section_pred[:, y:y+p.shape[1], x:x+p.shape[2]] = p[:, :, :]\n",
    "print((test_section_pred == test_section_label).all())\n",
    "'''\n",
    "'''\n",
    "for y_ in range(4):\n",
    "    for x_ in range(4):\n",
    "        x = x_*384\n",
    "        y = y_*384\n",
    "        pad = 18\n",
    "        p = np.expand_dims(test_label[1250-pad+x:1250-pad+384+x, 1250-pad+y:1250-pad+384+y, 0:64], 0)\n",
    "        \n",
    "        p = p[0, :, :, 10:10+50]\n",
    "\n",
    "        if x_ == 0:\n",
    "            p = p[pad:, :, :]\n",
    "        else:\n",
    "            x = x-pad\n",
    "        if x_ == 3:\n",
    "            p = p[:-pad, :, :]\n",
    "        if y_ == 0:\n",
    "            p = p[:, pad:, :]\n",
    "        else:\n",
    "            y = y-pad\n",
    "        if y_ == 3:\n",
    "            p = p[:, :-pad, :]\n",
    "            \n",
    "        test_section_pred[x:x+p.shape[0], y:y+p.shape[1], :] = p[:, :, :]\n",
    "print((test_section_pred == test_section_label).all())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(PATCH_SIZE) == 3 and PATCH_SIZE[-1] == 1:\n",
    "    # pred 2048 x 2048\n",
    "    test_section_pred.fill(0)\n",
    "    for z in range(50):\n",
    "        p = np.expand_dims(np.expand_dims(test_image[z+10, 1000:1000+2048, 1000:1000+2048], -1), 0)\n",
    "        p = model.predict(p)[0,250:250+1500,250:250+1500,0]\n",
    "        test_section_pred[z] = p\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_2048 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_2048 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_2048 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_2048 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_2048 = (mean_dist_contour_2048[0])[0]\n",
    "    mean_dist_pred_2048 = (mean_dist_contour_2048[1])[0]\n",
    "    cc_2048, cc_tp_2048, cc_fn_2048, cc_ud_2048 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    # pred 384 x 384\n",
    "    test_section_pred.fill(0)\n",
    "    for z in range(50):\n",
    "        for y_ in range(4):\n",
    "            for x_ in range(4):\n",
    "                x = x_*384\n",
    "                y = y_*384\n",
    "                pad = 18\n",
    "                p = np.expand_dims(np.expand_dims(test_image[z+10, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "                p = model.predict(p)[0, :, :, 0]\n",
    "\n",
    "                if x_ == 0:\n",
    "                    p = p[:, pad:]\n",
    "                else:\n",
    "                    x = x-pad\n",
    "                if x_ == 3:\n",
    "                    p = p[:, :-pad]\n",
    "                if y_ == 0:\n",
    "                    p = p[pad:, :]\n",
    "                else:\n",
    "                    y = y-pad\n",
    "                if y_ == 3:\n",
    "                    p = p[:-pad, :]\n",
    "\n",
    "                test_section_pred[z, y:y+p.shape[0], x:x+p.shape[1]] = p[:, :]\n",
    "        \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(modelname + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_2048) + \",\")\n",
    "    f_results.write(str(iou_2048) + \",\")\n",
    "    f_results.write(str(f1_2048) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_2048) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_2048) + \",\")\n",
    "    f_results.write(str(cc_tp_2048) + \",\")\n",
    "    f_results.write(str(cc_fn_2048) + \",\")\n",
    "    f_results.write(str(cc_ud_2048) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "\n",
    "elif len(PATCH_SIZE) == 4:\n",
    "    test_section_pred.fill(0)\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(np.expand_dims(test_image[0:64, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "            p = model.predict(p)[0,10:10+50, :, :, 0]\n",
    "\n",
    "            if x_ == 0:\n",
    "                p = p[:, :, pad:]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:, :, :-pad]\n",
    "            if y_ == 0:\n",
    "                p = p[:, pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:, :-pad, :]\n",
    "\n",
    "            test_section_pred[:, y:y+p.shape[1], x:x+p.shape[2]] = p[:, :, :]\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_384 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_384 = (mean_dist_contour_384[0])[0]\n",
    "    mean_dist_pred_384 = (mean_dist_contour_384[1])[0]\n",
    "    cc_384, cc_tp_384, cc_fn_384, cc_ud_384 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(modelname + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_384) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_384) + \",\")\n",
    "    f_results.write(str(cc_tp_384) + \",\")\n",
    "    f_results.write(str(cc_fn_384) + \",\")\n",
    "    f_results.write(str(cc_ud_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "\n",
    "elif len(PATCH_SIZE) == 3:\n",
    "    test_section_pred.fill(0)\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(test_label[1250-pad+x:1250-pad+384+x, 1250-pad+y:1250-pad+384+y, 0:64], 0)\n",
    "            p = model.predict(p)[0,:, :, 10:10+50]\n",
    "\n",
    "            if x_ == 0:\n",
    "                p = p[pad:, :, :]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:-pad, :, :]\n",
    "            if y_ == 0:\n",
    "                p = p[:, pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:, :-pad, :]\n",
    "\n",
    "            test_section_pred[x:x+p.shape[0], y:y+p.shape[1], :] = p[:, :, :]\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_384 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_384 = (mean_dist_contour_384[0])[0]\n",
    "    mean_dist_pred_384 = (mean_dist_contour_384[1])[0]\n",
    "    cc_384, cc_tp_384, cc_fn_384, cc_ud_384 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(modelname + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_384) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_384) + \",\")\n",
    "    f_results.write(str(cc_tp_384) + \",\")\n",
    "    f_results.write(str(cc_fn_384) + \",\")\n",
    "    f_results.write(str(cc_ud_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "    \n",
    "else:\n",
    "    print(\"error: patch size\", flush=True)\n",
    "    exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf230",
   "language": "python",
   "name": "venv_tf230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
