{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import skimage\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "\n",
    "import data.patch2D\n",
    "import data.patch25D\n",
    "import data.patch2DM\n",
    "import data.patch3D\n",
    "\n",
    "import models.UNet2D\n",
    "import models.ResUNet2D\n",
    "import models.UNet3D\n",
    "import models.UNet2DMM\n",
    "import models.EfficientUNet\n",
    "\n",
    "import metrics.connected_components\n",
    "import metrics.distance_contour\n",
    "\n",
    "from utils.clean_string import clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (64*64*16)/(256*256) == 1\n",
    "if 'ipykernel' in sys.modules:\n",
    "    verbose = 1\n",
    "    # params = [\"2DUNET-64\", \"(384, 384, 64)\", \"1\", \"MAE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-64\", \"(512, 512, 1)\", \"4\", \"MSE_DT_20_20\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-64\", \"(512, 512, 1)\", \"4\", \"MSE\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-32\", \"(256, 256, 16)\", \"1\", \"BCE\", \"MitoEM-H\"]\n",
    "    params = [\"3DUNET-32\", \"(32,256,256,1)\", \"1\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-64\", \"(512,512,1)\", \"4\", \"DICE\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-64\", \"(256,256,16)\", \"4\", \"JACCARD\", \"MitoEM-H\"]\n",
    "    # params = [\"3DUNET-32\", \"(32,256,256,1)\", \"1\", \"JACCARD\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-64\", \"(512,512,1)\", \"4\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-32\", \"(256,256,1)\", \"4\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "    # params = [\"2DUNET-32\", \"(1024,1024,1)\", \"2\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "    # params = [\"3DUNET-32\", \"(16,64,64,1)\", \"4\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "    # params = [\"3DUNET-32\", \"(32,256,256,1)\", \"2\", \"MSE_DT\", \"MitoEM-H\"]\n",
    "else:\n",
    "    verbose = 0\n",
    "    params = sys.argv[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"U2_U3_LOSS\"\n",
    "# EXP_NAME = \"U2_U3_U2MM_MODELS\"\n",
    "# EXP_NAME = \"U2_U3_U2MM_PATCH_SIZE\"\n",
    "# EXP_NAME = \"U2_U3_BEST\"\n",
    "\n",
    "# Exp params\n",
    "# params = [\"str_modelname\", \"tuple_input_size\", \"int_batch_size\", \"str_lossname\", \"str_dataset\"]\n",
    "# lossname = MSE, MSE_DT, MAE_DT, DICE, JACCARD, BCE, MSE_DT_20_20, MSE_DT_10_10, MSE_DT_5_5, MSE_DT_20_5\n",
    "N_PARAMS = 5\n",
    "if not (len(params) == N_PARAMS):\n",
    "    print(\"error: number of param\")\n",
    "    exit(1)\n",
    "\n",
    "MODELNAME = params[0]\n",
    "PATCH_SIZE = tuple(map(int, params[1].replace(\"(\",\"\").replace(\")\",\"\").split(','))) \n",
    "BATCH_SIZE = int(params[2])\n",
    "LOSS = str(params[3])\n",
    "DATASET = str(params[4])\n",
    "\n",
    "# Fixed params\n",
    "OUTPUT_CLASSES = 1\n",
    "\n",
    "OUTPUT_ACT = 'sigmoid'\n",
    "BINARY_THRESHOLD = 0.5\n",
    "if \"DT\" in LOSS:\n",
    "    OUTPUT_ACT = 'tanh'\n",
    "    BINARY_THRESHOLD = 0\n",
    "\n",
    "EPOCHS = 250\n",
    "TRAIN_PER_EPOCHS = 160\n",
    "VALID_PER_EPOCHS = 40\n",
    "EARLY_PATIENCE = 20\n",
    "\n",
    "# Exp related computing\n",
    "if   MODELNAME == \"2DUNET-8\":\n",
    "    # Total params: 488,689\n",
    "    MODEL = models.UNet2D.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=8, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"2DUNET-16\":\n",
    "    # Total params: 1,946,841\n",
    "    MODEL = models.UNet2D.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=16, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"2DUNET-32\":\n",
    "    # Total params: 7,771,561\n",
    "    MODEL = models.UNet2D.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=32, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"2DUNET-64\":\n",
    "    # Total params: 31,054,665\n",
    "    if PATCH_SIZE[-1] == 1:\n",
    "        MODEL = models.UNet2D.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "                 filters=64, depth=5, conv_per_block=2,\n",
    "                 dropouts=0.50, batch_normalization=True)\n",
    "    else:\n",
    "        MODEL = models.UNet2D.UNet(input_shape=(None, None, PATCH_SIZE[-1]), output_classes=PATCH_SIZE[-1], output_activation=OUTPUT_ACT,\n",
    "                 filters=64, depth=5, conv_per_block=2,\n",
    "                 dropouts=0.50, batch_normalization=True)\n",
    "\n",
    "elif MODELNAME == \"2DRESUNET-64\":\n",
    "    # Total params: 32,446,849\n",
    "    MODEL = models.ResUNet2D.RESUNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=64, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "\n",
    "elif MODELNAME == \"3DUNET-8\":\n",
    "    # Total params: 1,474,433\n",
    "    MODEL = models.UNet3D.UNet(input_shape=(None, None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=8, depth=5, pool_size=(2, 2, 2), conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"3DUNET-16_4\":\n",
    "    # Total params: 1,462,401\n",
    "    MODEL = models.UNet3D.UNet(input_shape=(None, None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=16, depth=4, pool_size=(2, 2, 2), conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"3DUNET-16\":\n",
    "    # Total params: 5,889,921\n",
    "    MODEL = models.UNet3D.UNet(input_shape=(None, None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=16, depth=5, pool_size=(2, 2, 2), conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"3DUNET-32_4\":\n",
    "    # Total params: 5,841,665\n",
    "    MODEL = models.UNet3D.UNet(input_shape=(None, None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=32, depth=4, pool_size=(2, 2, 2), conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"3DUNET-32\":\n",
    "    # Total params: 23,544,065\n",
    "    MODEL = models.UNet3D.UNet(input_shape=(None, None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=32, depth=5, pool_size=(2, 2, 2), conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "\n",
    "elif MODELNAME == \"2DUNETMM-8\":\n",
    "    # Total params: 979,441\n",
    "    MODEL = models.UNet2DMM.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=8, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"2DUNETMM-16\":\n",
    "    # Total params: 3,909,849\n",
    "    MODEL = models.UNet2DMM.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=16, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "elif MODELNAME == \"2DUNETMM-32\":\n",
    "    # Total params: 15,623,593\n",
    "    MODEL = models.UNet2DMM.UNet(input_shape=(None, None, 1), output_classes=OUTPUT_CLASSES, output_activation=OUTPUT_ACT,\n",
    "             filters=32, depth=5, conv_per_block=2,\n",
    "             dropouts=0.50, batch_normalization=True)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"error: model does not exist\")\n",
    "    exit(1)\n",
    "\n",
    "print(EXP_NAME, MODELNAME, PATCH_SIZE, BATCH_SIZE, LOSS, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"I3\":\n",
    "    import data.datasets.I3 as D\n",
    "elif DATASET == \"LW4\":\n",
    "    import data.datasets.LW4 as D\n",
    "elif DATASET == \"LW4_40_9\":\n",
    "    import data.datasets.LW4_40_9 as D\n",
    "elif DATASET == \"MitoEM\":\n",
    "    print(\"NOT IMPLEMENTED\")\n",
    "    exit()\n",
    "    if LOSS == \"MSE_DT\":\n",
    "        import data.datasets.MitoEM_dt as D\n",
    "    else:\n",
    "        import data.datasets.MitoEM as D\n",
    "elif DATASET == \"MitoEM-H\":\n",
    "    if \"DT\" in LOSS:\n",
    "        import data.datasets.MitoEMH_dt as D\n",
    "        s = LOSS.split(\"_\")\n",
    "        if len(s) == 4:\n",
    "            D.load_dt(int(s[2]), int(s[3]))\n",
    "        else:\n",
    "            D.load_dt(2, 2)\n",
    "    else:\n",
    "        import data.datasets.MitoEMH as D\n",
    "elif DATASET == \"MitoEM-R\":\n",
    "    if \"DT\" in LOSS:\n",
    "        import data.datasets.MitoEMR_dt as D\n",
    "    else:\n",
    "        import data.datasets.MitoEMR as D\n",
    "else:\n",
    "    print(\"error: dataset does not exist\")\n",
    "    exit(1)\n",
    "\n",
    "if(os.uname()[1] == 'lythandas'):\n",
    "    OUTPUT_FOLDER = \"/home/cyril/Development/NeNISt/\" + EXP_NAME\n",
    "else:\n",
    "    OUTPUT_FOLDER = \"/b/home/miv/cmeyer/NeNISt/\" + EXP_NAME\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.today().strftime(\"%j%H%M%S%f\")[:-2]\n",
    "EXP_NAME = (EXP_NAME + \"_\" + str(MODELNAME) + \"_\" + str(PATCH_SIZE) + \"_\" + LOSS + \"_\" + DATASET + \"_\" + dt).replace(\" \", \"\")\n",
    "print(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_image = D.train_image_normalized_f32\n",
    "train_labels_dt = D.train_labels_dt\n",
    "train_labels_indexes = [D.train_label_1_indexes, D.train_label_2_indexes, D.train_label_3_indexes, D.train_label_4_indexes, D.train_label_5_indexes, D.train_label_6_indexes, D.train_label_7_indexes, D.train_label_8_indexes, D.train_label_9_indexes]\n",
    "\n",
    "test_image = D.test_image_normalized_f32\n",
    "test_labels_dt = D.test_labels_dt\n",
    "test_labels_indexes = [D.test_label_1_indexes, D.test_label_2_indexes, D.test_label_3_indexes, D.test_label_4_indexes, D.test_label_5_indexes, D.test_label_6_indexes, D.test_label_7_indexes, D.test_label_8_indexes, D.test_label_9_indexes]\n",
    "'''\n",
    "\n",
    "train_image = D.train_image_normalized_f16\n",
    "train_label = D.train_label\n",
    "valid_image = D.valid_image_normalized_f16\n",
    "valid_label = D.valid_label\n",
    "\n",
    "# case when X and Z axis are echanged\n",
    "if not PATCH_SIZE[-1] == 1:\n",
    "    train_image = np.moveaxis(train_image, 0, 2)\n",
    "    train_label = np.moveaxis(train_label, 0, 2)\n",
    "    valid_image = np.moveaxis(valid_image, 0, 2)\n",
    "    valid_label = np.moveaxis(valid_label, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weights\n",
    "'''\n",
    "# compute class weights on train dataset\n",
    "train_labels_one_hot = train_labels_dt > 0\n",
    "\n",
    "# class weight ([0, 1])\n",
    "class_weights = np.zeros(train_labels_one_hot.shape[-1])\n",
    "for c in range(len(class_weights)):\n",
    "    class_weights[c] = 1 - train_labels_one_hot[:,:,:,c].sum() / (train_labels_one_hot.shape[0] * train_labels_one_hot.shape[1] * train_labels_one_hot.shape[2])\n",
    "print(np.round(class_weights, 2))\n",
    "\n",
    "# class weight : wj=n_samples / (n_classes * n_samplesj)\n",
    "class_weights = np.zeros(train_labels_one_hot.shape[-1])\n",
    "for c in range(len(class_weights)):\n",
    "    class_weights[c] = (train_labels_one_hot.shape[0] * train_labels_one_hot.shape[1] * train_labels_one_hot.shape[2]) / (train_labels_one_hot.shape[3] * train_labels_one_hot[:,:,:,c].sum())\n",
    "print(np.round(class_weights, 2))\n",
    "'''\n",
    "# only run once, results :\n",
    "# BCE WEIGHTS: [0.52861168 9.23769132]\n",
    "'''\n",
    "if LOSS == \"BCE\":\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "                                            classes=[0,1],\n",
    "                                            y=train_label[0:10].flatten())\n",
    "    print(\"BCE WEIGHTS:\", weights)\n",
    "'''\n",
    "\n",
    "weights = None\n",
    "if LOSS == \"BCE\":\n",
    "    weights = np.array([0.5, 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "'''\n",
    "if \"3D\" in MODELNAME and len(PATCH_SIZE) == 4:\n",
    "    train = data.patch3D.gen_patches_batch_augmented_3d_label_indexes_one_hot(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], train_image, train_labels_dt, train_labels_indexes, batch_size=BATCH_SIZE)\n",
    "    test = data.patch3D.gen_patches_batch_augmented_3d_label_indexes_one_hot(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], test_image, test_labels_dt, test_labels_indexes, batch_size=BATCH_SIZE)\n",
    "elif \"2D\" in MODELNAME and len(PATCH_SIZE) == 3:\n",
    "    if PATCH_SIZE[0] == PATCH_SIZE[1]:\n",
    "        train = data.patch2D.gen_patches_batch_augmented_label_indexes_one_hot(PATCH_SIZE[0], train_image, train_labels_dt, train_labels_indexes, batch_size=BATCH_SIZE)\n",
    "        test = data.patch2D.gen_patches_batch_augmented_label_indexes_one_hot(PATCH_SIZE[0], test_image, test_labels_dt, test_labels_indexes, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        print(\"error: non square 2D patch size, check data.patch2D\")\n",
    "        exit(1)\n",
    "else:\n",
    "    print(\"error: patch size the model are not compatible\")\n",
    "    exit(1)\n",
    "'''\n",
    "if len(PATCH_SIZE) == 3:\n",
    "    if PATCH_SIZE[-1] == 1:\n",
    "        train = data.patch2D.gen_patches_batch_augmented_2d_bin(PATCH_SIZE[0], PATCH_SIZE[1], train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch2D.gen_patches_batch_augmented_2d_bin(PATCH_SIZE[0], PATCH_SIZE[1], valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    else:\n",
    "        train = data.patch3D.gen_patches_batch_augmented_3d_bin_nochan(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "        valid = data.patch3D.gen_patches_batch_augmented_3d_bin_nochan(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "elif len(PATCH_SIZE) == 4:\n",
    "    train = data.patch3D.gen_patches_batch_augmented_3d_bin(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], train_image, train_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "    valid = data.patch3D.gen_patches_batch_augmented_3d_bin(PATCH_SIZE[0], PATCH_SIZE[1], PATCH_SIZE[2], valid_image, valid_label, batch_size=BATCH_SIZE, weights=weights)\n",
    "else:\n",
    "    print(\"error: patch size\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "from tensorflow.keras import backend as K\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    # https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    # https://gist.github.com/wassname/7793e2058c5c9dacb5212c0ac0b18a8a\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "if \"MSE\" in LOSS:\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "elif \"MAE_DT\" in LOSS:\n",
    "    loss = tf.keras.losses.MeanAbsoluteError()\n",
    "elif LOSS == \"DICE\":\n",
    "    loss = dice_coef_loss\n",
    "elif LOSS == \"JACCARD\":\n",
    "    loss = jaccard_distance_loss\n",
    "elif LOSS == \"SM_DICE\":\n",
    "    loss = sm.losses.dice_loss\n",
    "elif LOSS == \"SM_JACCARD\":\n",
    "    loss = sm.losses.jaccard_loss\n",
    "elif LOSS == \"SM_BIN_FOCAL\":\n",
    "    loss = sm.losses.binary_focal_loss\n",
    "elif LOSS == \"BCE\":\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "else:\n",
    "    print(\"error: loss name\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "model = MODEL\n",
    "model.compile(optimizer=optimizer, loss=loss) # metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.BinaryCrossentropy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience=EARLY_PATIENCE, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# fast test\n",
    "if 'ipykernel' in sys.modules:\n",
    "    EPOCHS = 10\n",
    "    TRAIN_PER_EPOCHS = 64\n",
    "    VALID_PER_EPOCHS = 6\n",
    "\n",
    "fit_history = model.fit(train, steps_per_epoch=TRAIN_PER_EPOCHS, epochs=EPOCHS,\n",
    "                        validation_data=valid, validation_steps=VALID_PER_EPOCHS,\n",
    "                        verbose=verbose, callbacks=[earlystopping])\n",
    "\n",
    "t1 = time.time()\n",
    "train_time = int(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(OUTPUT_FOLDER + \"/\" + EXP_NAME + \".h5\")\n",
    "# model.evaluate(valid, steps=TRAIN_PER_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = len(fit_history.history['loss'])\n",
    "f_history_name = OUTPUT_FOLDER + \"/\" + clean_string(EXP_NAME + \"_history_\" + str(n_epochs)) + \".txt\"\n",
    "f = open(f_history_name, \"w\")\n",
    "f.write(str(fit_history.history))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_results_name = OUTPUT_FOLDER + \"/\" + clean_string(EXP_NAME) + \".csv\"\n",
    "f_results = open(f_results_name, \"w\")\n",
    "if len(PATCH_SIZE) == 3 and PATCH_SIZE[-1] == 1:\n",
    "    f_results.write(\"modelname,train_time,ACC_2048,IOU_2048,F1_2048,MEAN_DIST_GD_2048,MEAN_DIST_PRED_2048,CC_TP_2048,CC_FN_2048,CC_UD_2048,ACC_384,IOU_384,F1_384\\n\")\n",
    "elif len(PATCH_SIZE) == 3:\n",
    "    f_results.write(\"modelname,train_time,ACC_384,IOU_384,F1_384,MEAN_DIST_GD_384,MEAN_DIST_PRED_384,CC_TP_384,CC_FN_384,CC_UD_384\\n\")\n",
    "elif len(PATCH_SIZE) == 4:\n",
    "    f_results.write(\"modelname,train_time,ACC_384,IOU_384,F1_384,MEAN_DIST_GD_384,MEAN_DIST_PRED_384,CC_TP_384,CC_FN_384,CC_UD_384\\n\")\n",
    "f_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = valid_image\n",
    "test_label = valid_label\n",
    "if PATCH_SIZE[-1] == 1:\n",
    "    test_section_label = test_label[10:10+50, 1250:1250+1500, 1250:1250+1500]\n",
    "    test_section_pred  = np.zeros(test_section_label.shape, dtype=test_section_label.dtype)\n",
    "else:\n",
    "    test_section_label = test_label[1250:1250+1500, 1250:1250+1500, 10:10+50]\n",
    "    test_section_pred  = np.zeros(test_section_label.shape, dtype=test_section_label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof\n",
    "'''\n",
    "for z in range(50):\n",
    "    p = np.expand_dims(np.expand_dims(test_label[z+10, 1000:1000+2048, 1000:1000+2048], -1), 0)\n",
    "    p = p[0,250:250+1500,250:250+1500,0]\n",
    "    test_section_pred[z] = p\n",
    "print((test_section_pred == test_section_label).all())\n",
    "\n",
    "for z in range(50):\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(np.expand_dims(test_label[z+10, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "            p = p[0, :, :, 0]\n",
    "            \n",
    "            if x_ == 0:\n",
    "                p = p[:, pad:]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:, :-pad]\n",
    "            if y_ == 0:\n",
    "                p = p[pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:-pad, :]\n",
    "\n",
    "            test_section_pred[z, y:y+p.shape[0], x:x+p.shape[1]] = p[:, :]\n",
    "\n",
    "print((test_section_pred == test_section_label).all())\n",
    "\n",
    "\n",
    "for y_ in range(4):\n",
    "    for x_ in range(4):\n",
    "        x = x_*384\n",
    "        y = y_*384\n",
    "        pad = 18\n",
    "        p = np.expand_dims(np.expand_dims(test_label[0:64, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "        \n",
    "        p = p[0,10:10+50, :, :, 0]\n",
    "\n",
    "        if x_ == 0:\n",
    "            p = p[:, :, pad:]\n",
    "        else:\n",
    "            x = x-pad\n",
    "        if x_ == 3:\n",
    "            p = p[:, :, :-pad]\n",
    "        if y_ == 0:\n",
    "            p = p[:, pad:, :]\n",
    "        else:\n",
    "            y = y-pad\n",
    "        if y_ == 3:\n",
    "            p = p[:, :-pad, :]\n",
    "        \n",
    "        test_section_pred[:, y:y+p.shape[1], x:x+p.shape[2]] = p[:, :, :]\n",
    "print((test_section_pred == test_section_label).all())\n",
    "'''\n",
    "'''\n",
    "for y_ in range(4):\n",
    "    for x_ in range(4):\n",
    "        x = x_*384\n",
    "        y = y_*384\n",
    "        pad = 18\n",
    "        p = np.expand_dims(test_label[1250-pad+x:1250-pad+384+x, 1250-pad+y:1250-pad+384+y, 0:64], 0)\n",
    "        \n",
    "        p = p[0, :, :, 10:10+50]\n",
    "\n",
    "        if x_ == 0:\n",
    "            p = p[pad:, :, :]\n",
    "        else:\n",
    "            x = x-pad\n",
    "        if x_ == 3:\n",
    "            p = p[:-pad, :, :]\n",
    "        if y_ == 0:\n",
    "            p = p[:, pad:, :]\n",
    "        else:\n",
    "            y = y-pad\n",
    "        if y_ == 3:\n",
    "            p = p[:, :-pad, :]\n",
    "            \n",
    "        test_section_pred[x:x+p.shape[0], y:y+p.shape[1], :] = p[:, :, :]\n",
    "print((test_section_pred == test_section_label).all())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(PATCH_SIZE) == 3 and PATCH_SIZE[-1] == 1:\n",
    "    # pred 2048 x 2048\n",
    "    test_section_pred.fill(0)\n",
    "    for z in range(50):\n",
    "        p = np.expand_dims(np.expand_dims(test_image[z+10, 1000:1000+2048, 1000:1000+2048], -1), 0)\n",
    "        p = model.predict(p)[0,250:250+1500,250:250+1500,0]\n",
    "        test_section_pred[z] = p\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_2048 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_2048 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_2048 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_2048 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_2048 = (mean_dist_contour_2048[0])[0]\n",
    "    mean_dist_pred_2048 = (mean_dist_contour_2048[1])[0]\n",
    "    cc_2048, cc_tp_2048, cc_fn_2048, cc_ud_2048 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    # pred 384 x 384\n",
    "    test_section_pred.fill(0)\n",
    "    for z in range(50):\n",
    "        for y_ in range(4):\n",
    "            for x_ in range(4):\n",
    "                x = x_*384\n",
    "                y = y_*384\n",
    "                pad = 18\n",
    "                p = np.expand_dims(np.expand_dims(test_image[z+10, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "                p = model.predict(p)[0, :, :, 0]\n",
    "\n",
    "                if x_ == 0:\n",
    "                    p = p[:, pad:]\n",
    "                else:\n",
    "                    x = x-pad\n",
    "                if x_ == 3:\n",
    "                    p = p[:, :-pad]\n",
    "                if y_ == 0:\n",
    "                    p = p[pad:, :]\n",
    "                else:\n",
    "                    y = y-pad\n",
    "                if y_ == 3:\n",
    "                    p = p[:-pad, :]\n",
    "\n",
    "                test_section_pred[z, y:y+p.shape[0], x:x+p.shape[1]] = p[:, :]\n",
    "        \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(MODELNAME + \"_\" + LOSS + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_2048) + \",\")\n",
    "    f_results.write(str(iou_2048) + \",\")\n",
    "    f_results.write(str(f1_2048) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_2048) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_2048) + \",\")\n",
    "    f_results.write(str(cc_tp_2048) + \",\")\n",
    "    f_results.write(str(cc_fn_2048) + \",\")\n",
    "    f_results.write(str(cc_ud_2048) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "\n",
    "elif len(PATCH_SIZE) == 4:\n",
    "    test_section_pred.fill(0)\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(np.expand_dims(test_image[0:64, 1250-pad+y:1250-pad+384+y, 1250-pad+x:1250-pad+384+x], -1), 0)\n",
    "            p = model.predict(p)[0,10:10+50, :, :, 0]\n",
    "\n",
    "            if x_ == 0:\n",
    "                p = p[:, :, pad:]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:, :, :-pad]\n",
    "            if y_ == 0:\n",
    "                p = p[:, pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:, :-pad, :]\n",
    "\n",
    "            test_section_pred[:, y:y+p.shape[1], x:x+p.shape[2]] = p[:, :, :]\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_384 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_384 = (mean_dist_contour_384[0])[0]\n",
    "    mean_dist_pred_384 = (mean_dist_contour_384[1])[0]\n",
    "    cc_384, cc_tp_384, cc_fn_384, cc_ud_384 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(MODELNAME + \"_\" + LOSS + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_384) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_384) + \",\")\n",
    "    f_results.write(str(cc_tp_384) + \",\")\n",
    "    f_results.write(str(cc_fn_384) + \",\")\n",
    "    f_results.write(str(cc_ud_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "\n",
    "elif len(PATCH_SIZE) == 3:\n",
    "    test_section_pred.fill(0)\n",
    "    for y_ in range(4):\n",
    "        for x_ in range(4):\n",
    "            x = x_*384\n",
    "            y = y_*384\n",
    "            pad = 18\n",
    "            p = np.expand_dims(test_label[1250-pad+x:1250-pad+384+x, 1250-pad+y:1250-pad+384+y, 0:64], 0)\n",
    "            p = model.predict(p)[0,:, :, 10:10+50]\n",
    "\n",
    "            if x_ == 0:\n",
    "                p = p[pad:, :, :]\n",
    "            else:\n",
    "                x = x-pad\n",
    "            if x_ == 3:\n",
    "                p = p[:-pad, :, :]\n",
    "            if y_ == 0:\n",
    "                p = p[:, pad:, :]\n",
    "            else:\n",
    "                y = y-pad\n",
    "            if y_ == 3:\n",
    "                p = p[:, :-pad, :]\n",
    "\n",
    "            test_section_pred[x:x+p.shape[0], y:y+p.shape[1], :] = p[:, :, :]\n",
    "    \n",
    "    test_section_label_f = (test_section_label>BINARY_THRESHOLD).flatten()\n",
    "    test_section_pred_f = (test_section_pred>BINARY_THRESHOLD).flatten()\n",
    "    acc_384 = sklearn.metrics.accuracy_score(test_section_label_f, test_section_pred_f)\n",
    "    iou_384 = sklearn.metrics.jaccard_score(test_section_label_f, test_section_pred_f)\n",
    "    f1_384 = sklearn.metrics.f1_score(test_section_label_f, test_section_pred_f)\n",
    "    mean_dist_contour_384 = metrics.distance_contour.distance_contour_segmentation_3D((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD))\n",
    "    mean_dist_gd_384 = (mean_dist_contour_384[0])[0]\n",
    "    mean_dist_pred_384 = (mean_dist_contour_384[1])[0]\n",
    "    cc_384, cc_tp_384, cc_fn_384, cc_ud_384 = metrics.connected_components.connected_components_detection((test_section_label>BINARY_THRESHOLD), (test_section_pred>BINARY_THRESHOLD), 0.75)\n",
    "    \n",
    "    \n",
    "    f_results = open(f_results_name, \"a\")\n",
    "    f_results.write(MODELNAME + \"_\" + LOSS + \",\")\n",
    "    f_results.write(str(train_time) + \",\")\n",
    "    f_results.write(str(acc_384) + \",\")\n",
    "    f_results.write(str(iou_384) + \",\")\n",
    "    f_results.write(str(f1_384) + \",\")\n",
    "    f_results.write(str(mean_dist_gd_384) + \",\")\n",
    "    f_results.write(str(mean_dist_pred_384) + \",\")\n",
    "    f_results.write(str(cc_tp_384) + \",\")\n",
    "    f_results.write(str(cc_fn_384) + \",\")\n",
    "    f_results.write(str(cc_ud_384) + \"\\n\")\n",
    "    f_results.close()\n",
    "    \n",
    "else:\n",
    "    print(\"error: patch size\")\n",
    "    exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf230",
   "language": "python",
   "name": "venv_tf230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
